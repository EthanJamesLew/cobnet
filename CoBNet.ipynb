{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c86285a8-f9cd-4317-b31c-81becad5aa3f",
   "metadata": {},
   "source": [
    "## Problem Statement\n",
    "\n",
    "Consider a nonlinear dynamical system characterized by its state $x \\in \\mathbb{R}^n$ and the corresponding vector field $F(x) \\in \\mathbb{R}^n$. Given a set of $N$ snapshots of the system's state, denoted as $\\{x_1, x_2, \\dots, x_N\\}$, along with their associated time derivatives $\\{\\dot{x}_1, \\dot{x}_2, \\dots, \\dot{x}_N\\}$, where $\\dot{x}_i = F(x_i)$, we seek to learn a mapping $\\alpha(x): \\mathbb{R}^n \\rightarrow \\mathbb{R}^m$, a linear transformation matrix $A \\in \\mathbb{R}^{m \\times m}$, and an offset vector $b \\in \\mathbb{R}^m$ such that the relationship\n",
    "\n",
    "\\begin{equation}\n",
    "J_\\alpha(x_i) \\cdot \\dot{x}_i = A \\cdot \\alpha(x_i) + b\n",
    "\\end{equation}\n",
    "\n",
    "holds for all $i = 1, 2, \\dots, N$. Here, $J_\\alpha(x)$ denotes the Jacobian matrix of the mapping $\\alpha(x)$ with respect to the state $x$.\n",
    "\n",
    "This constraint can be approximated by minimizing the mean squared error between the left-hand side and the right-hand side of the equation across all training examples. The optimization objective is therefore defined as\n",
    "\n",
    "\\begin{equation}\n",
    "\\min_{\\alpha, A, b} \\frac{1}{N} \\sum_{i=1}^N \\left\\| J_\\alpha(x_i) \\cdot \\dot{x}_i - \\left( A \\cdot \\alpha(x_i) + b \\right) \\right\\|_2^2,\n",
    "\\end{equation}\n",
    "\n",
    "where $\\|\\cdot\\|_2$ denotes the Euclidean norm. The learned transformation $\\alpha(x)$ aims to map the original nonlinear dynamics into a space where they can be approximated by an affine system.\n",
    "\n",
    "## Architecture\n",
    "\n",
    "The architecture consists of two main components: a neural network that learns a mapping $\\alpha(x)$ from the original state space to a higher-dimensional space, and a learned linear operator $\\mathbf{A}$ with an offset vector $\\mathbf{b}$, which operates in this higher-dimensional space. \n",
    "\n",
    "### Mapping Network\n",
    "\n",
    "\n",
    "The mapping network $\\alpha(x)$ is a feedforward neural network that transforms the input state vector $x \\in \\mathbb{R}^n$ into a higher-dimensional space $\\mathbb{R}^m$. The network consists of four fully connected layers:\n",
    "\n",
    "- **Layer 1**: A fully connected layer with 128 neurons, followed by a ReLU activation function.\n",
    "- **Layer 2**: A fully connected layer with 64 neurons, followed by a ReLU activation function.\n",
    "- **Layer 3**: A fully connected layer with 64 neurons, followed by a ReLU activation function.\n",
    "- **Output Layer**: A fully connected layer that outputs the final mapping vector $\\alpha(x) \\in \\mathbb{R}^m$.\n",
    "\n",
    "The input to the network is first normalized using Batch Normalization. The activation function used in the hidden layers is ReLU, which introduces nonlinearity into the network. The output layer provides the mapped high-dimensional representation $\\alpha(x)$ without an activation function, ensuring that the output can take any value in $\\mathbb{R}^m$.\n",
    "\n",
    "### Linear Operator and Offset\n",
    "\n",
    "This transformation is mathematically described as:\n",
    "\n",
    "\\begin{equation}\n",
    "\\mathbf{A} \\alpha(x) + \\mathbf{b}\n",
    "\\end{equation}\n",
    "\n",
    "### Asymmetric Learning\n",
    "\n",
    "The learning process is asymmetric, employing a differential learning approach where one network (the mapping network) is optimized more intensively than the other (the linear operator and offset). This is because the mapping network $\\alpha(x)$ is responsible for transforming the nonlinear dynamics into a space where they can be more easily modeled as an affine system, which is a more complex task.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50f8fe94-74b0-4aef-a0dd-28f71119c36a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "def F(state):\n",
    "    \"\"\"\n",
    "    The Lorenz attractor nonlinear flow F(x)\n",
    "\n",
    "    TODO: use autokoopman to integration\n",
    "    \"\"\"\n",
    "    sigma = 10.0\n",
    "    rho = 28.0\n",
    "    beta = 8.0 / 3.0\n",
    "    \n",
    "    x = state[:, 0]\n",
    "    y = state[:, 1]\n",
    "    z = state[:, 2]\n",
    "    \n",
    "    dxdt = sigma * (y - x)\n",
    "    dydt = x * (rho - z) - y\n",
    "    dzdt = x * y - beta * z\n",
    "    \n",
    "    return torch.stack([dxdt, dydt, dzdt], dim=1)\n",
    "\n",
    "    \n",
    "class MappingNet(nn.Module):\n",
    "    \"\"\"\n",
    "    The neural network for a(x) with high-dimensional output\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim, output_dim=32):  # 32-dimensional a(x)\n",
    "        super(MappingNet, self).__init__()\n",
    "        self.normalizer = nn.BatchNorm1d(input_dim)  # Input normalizer\n",
    "        self.fc1 = nn.Linear(input_dim, 128)\n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        self.fc3 = nn.Linear(64, 64)\n",
    "        self.fco = nn.Linear(64, output_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.normalizer(x)  # Normalize input\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = torch.relu(self.fc3(x))\n",
    "        a = self.fco(x)\n",
    "        return a\n",
    "\n",
    "\n",
    "class LinearParams(nn.Module):\n",
    "    \"\"\"\n",
    "    The learned linear operator A and offset b\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim):  # Map from 32-dimensional a(x) to 3-dimensional x\n",
    "        super(LinearParams, self).__init__()\n",
    "        self.A = nn.Parameter(torch.randn(input_dim, input_dim))\n",
    "        self.b = nn.Parameter(torch.randn(input_dim))\n",
    "        \n",
    "    def forward(self, a):\n",
    "        return torch.matmul(self.A, a.T).T + self.b\n",
    "\n",
    "\n",
    "def compute_jacobian(network, x):\n",
    "    \"\"\"Ja(x)\"\"\"\n",
    "    # Make sure x requires gradients\n",
    "    x = x.clone().detach().requires_grad_(True)\n",
    "    \n",
    "    # Forward pass to get the output a(x)\n",
    "    a = network(x)\n",
    "    \n",
    "    # Initialize an empty list to store the Jacobian\n",
    "    jacobian = []\n",
    "    \n",
    "    # Compute the Jacobian for each output with respect to the input\n",
    "    for i in range(a.shape[1]):\n",
    "        # Create a tensor of the same shape as a, filled with zeros except for the i-th column\n",
    "        grad_output = torch.zeros_like(a)\n",
    "        grad_output[:, i] = 1.0\n",
    "        \n",
    "        # Compute the gradient of the i-th output w.r.t. x\n",
    "        jac_i = torch.autograd.grad(outputs=a, inputs=x,\n",
    "                                    grad_outputs=grad_output, create_graph=True)[0]\n",
    "        \n",
    "        jacobian.append(jac_i)\n",
    "    \n",
    "    # Stack the computed gradients along a new dimension to form the Jacobian\n",
    "    return torch.stack(jacobian, dim=1)  # Shape: (batch_size, a_dim, input_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de1961d2-7f04-4b3d-b368-e632a1d61019",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use this for tensorboard\n",
    "#!pip install tensorboard\n",
    "#%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8046b6b5-7bab-4428-b102-69bcb2eefe51",
   "metadata": {},
   "outputs": [],
   "source": [
    "%tensorboard --logdir=runs\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "#from torch.utils.tensorboard import notebook\n",
    "\n",
    "# Initialize the writer\n",
    "writer = SummaryWriter()\n",
    "\n",
    "# Initialize dimensions\n",
    "input_dim = 3  # Lorenz system state: (x, y, z)\n",
    "a_dim = 64    # High-dimensional a(x)\n",
    "output_dim = 3 # Output dimension should match the state dimension\n",
    "\n",
    "# Initialize models\n",
    "mapping_net = MappingNet(input_dim, output_dim=a_dim)\n",
    "linear_params = LinearParams(input_dim=a_dim)\n",
    "\n",
    "# Optimizers for both parts\n",
    "optimizer_a = optim.Adam(mapping_net.parameters(), lr=1e-4)\n",
    "optimizer_linear = optim.Adam(linear_params.parameters(), lr=1e-4)\n",
    "\n",
    "# Loss function\n",
    "mse_loss = nn.MSELoss()\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 3000\n",
    "mapping_steps = 5  # Number of mapping optimizer steps per epoch\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "\n",
    "    # Generate more random input state data\n",
    "    x = torch.randn((256, input_dim), requires_grad=True)  # More states (x, y, z) samples\n",
    "    \n",
    "    for _ in range(mapping_steps):\n",
    "        # Step 1: Optimize A and b\n",
    "        optimizer_a.zero_grad()\n",
    "        \n",
    "        # Forward pass through the network\n",
    "        a = mapping_net(x)\n",
    "        \n",
    "        # Recalculate the Jacobian of a(x)\n",
    "        #Ja = compute_jacobian(a, x)\n",
    "        Ja = compute_jacobian(mapping_net, x)\n",
    "        \n",
    "        # Recalculate the left and right sides\n",
    "        left_side = torch.einsum('bij,bj->bi', Ja, F(x))\n",
    "        right_side = linear_params(a)\n",
    "        #left_side = F(x)\n",
    "        #right_side =  torch.einsum('bij,bj->bi', pinv(Ja), linear_params(mapping_net(x)))\n",
    "        \n",
    "        # Compute the loss for a(x)\n",
    "        loss = mse_loss(left_side, right_side)\n",
    "        loss.backward(retain_graph=True)\n",
    "        optimizer_a.step()\n",
    "    \n",
    "    # Step 2: Optimize a(x)\n",
    "    optimizer_linear.zero_grad()\n",
    "    \n",
    "    # Forward pass through the network\n",
    "    a = mapping_net(x)\n",
    "    \n",
    "    # Recalculate the Jacobian of a(x)\n",
    "    #Ja = compute_jacobian(a, x)\n",
    "    Ja = compute_jacobian(mapping_net, x)\n",
    "    \n",
    "    # Recalculate the left and right sides\n",
    "    left_side = torch.einsum('bij,bj->bi', Ja, F(x))\n",
    "    right_side = linear_params(a)\n",
    "    #left_side = F(x)\n",
    "    #right_side =  torch.einsum('bij,bj->bi', pinv(Ja), linear_params(mapping_net(x)))\n",
    "    \n",
    "    # Compute the loss for a(x)\n",
    "    loss_a = mse_loss(left_side, right_side)\n",
    "    loss_a.backward(retain_graph=True)\n",
    "    optimizer_linear.step()\n",
    "\n",
    "    #Javi = pinv(compute_jacobian(mapping_net(xv), xv))\n",
    "    #left_side = torch.einsum('bij,bj->bi', Javi, mapping_net(xv))\n",
    "\n",
    "    #print(left_side, F(xv))\n",
    "    \n",
    "    # Print loss every 100 epochs\n",
    "    #if epoch % 100 == 0:\n",
    "    #    print(f'Epoch {epoch}, Loss (a): {loss_a.item()}')\n",
    "\n",
    "    writer.add_scalar('Loss/train', loss_a.item(), epoch)\n",
    "\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea0640c8-9016-4ae2-9e4e-d9ddc3c8d2a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare against random values\n",
    "x = torch.randn((256, input_dim), requires_grad=True)\n",
    "res =  torch.einsum('bij,bj->bi', (compute_jacobian(mapping_net, x)), F(x)) - linear_params(mapping_net(x))\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87ec97ac-6331-46f6-836b-2b7170251991",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "50f8fe94-74b0-4aef-a0dd-28f71119c36a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss (a): 0.737454891204834\n",
      "Epoch 100, Loss (a): 0.0012763711856678128\n",
      "Epoch 200, Loss (a): 0.00027541464078240097\n",
      "Epoch 300, Loss (a): 9.676031913841143e-05\n",
      "Epoch 400, Loss (a): 4.28910534537863e-05\n",
      "Epoch 500, Loss (a): 3.9558606658829376e-05\n",
      "Epoch 600, Loss (a): 6.744704296579584e-05\n",
      "Epoch 700, Loss (a): 1.4464032574323937e-05\n",
      "Epoch 800, Loss (a): 1.6896747183636762e-05\n",
      "Epoch 900, Loss (a): 8.137214172165841e-05\n",
      "Epoch 1000, Loss (a): 3.5528380976757035e-05\n",
      "Epoch 1100, Loss (a): 5.8680379879660904e-06\n",
      "Epoch 1200, Loss (a): 6.668429705314338e-06\n",
      "Epoch 1300, Loss (a): 5.373057138058357e-05\n",
      "Epoch 1400, Loss (a): 9.832109753915574e-06\n",
      "Epoch 1500, Loss (a): 2.1224175725365058e-05\n",
      "Epoch 1600, Loss (a): 7.894050213508308e-06\n",
      "Epoch 1700, Loss (a): 0.0005194611730985343\n",
      "Epoch 1800, Loss (a): 0.0001251824724022299\n",
      "Epoch 1900, Loss (a): 2.4196753656724468e-05\n",
      "Epoch 2000, Loss (a): 5.1929091569036245e-06\n",
      "Epoch 2100, Loss (a): 1.0573543477221392e-05\n",
      "Epoch 2200, Loss (a): 2.129108543158509e-05\n",
      "Epoch 2300, Loss (a): 0.004690155852586031\n",
      "Epoch 2400, Loss (a): 0.0015950866509228945\n",
      "Epoch 2500, Loss (a): 9.289896297559608e-06\n",
      "Epoch 2600, Loss (a): 4.1019734453584533e-07\n",
      "Epoch 2700, Loss (a): 1.9564382114367618e-07\n",
      "Epoch 2800, Loss (a): 3.0254715966293588e-05\n",
      "Epoch 2900, Loss (a): 7.3667638389451895e-06\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Define the Lorenz attractor nonlinear flow F(x)\n",
    "def F(state):\n",
    "    sigma = 10.0\n",
    "    rho = 28.0\n",
    "    beta = 8.0 / 3.0\n",
    "    \n",
    "    x = state[:, 0]\n",
    "    y = state[:, 1]\n",
    "    z = state[:, 2]\n",
    "    \n",
    "    dxdt = sigma * (y - x)\n",
    "    dydt = x * (rho - z) - y\n",
    "    dzdt = x * y - beta * z\n",
    "    \n",
    "    return torch.stack([dxdt, dydt, dzdt], dim=1)\n",
    "\n",
    "# Define the neural network for a(x) with high-dimensional output\n",
    "class MappingNet(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim=32):  # 32-dimensional a(x)\n",
    "        super(MappingNet, self).__init__()\n",
    "        self.normalizer = nn.BatchNorm1d(input_dim)  # Input normalizer\n",
    "        self.fc1 = nn.Linear(input_dim, 128)\n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        self.fc3 = nn.Linear(64, 64)\n",
    "        self.fco = nn.Linear(64, output_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.normalizer(x)  # Normalize input\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = torch.relu(self.fc3(x))\n",
    "        a = self.fco(x)\n",
    "        return a\n",
    "\n",
    "# Define the learned linear operator A and offset b\n",
    "class LinearParams(nn.Module):\n",
    "    def __init__(self, input_dim):  # Map from 32-dimensional a(x) to 3-dimensional x\n",
    "        super(LinearParams, self).__init__()\n",
    "        self.A = nn.Parameter(torch.randn(input_dim, input_dim))\n",
    "        self.b = nn.Parameter(torch.randn(input_dim))\n",
    "        \n",
    "    def forward(self, a):\n",
    "        return torch.matmul(self.A, a.T).T + self.b\n",
    "\n",
    "def compute_jacobian(a, x):\n",
    "    jacobian = []\n",
    "    for i in range(a.shape[1]):\n",
    "        grad_output = torch.zeros_like(a)\n",
    "        grad_output[:, i] = 1.0\n",
    "        jac_i = torch.autograd.grad(a, x, grad_outputs=grad_output, create_graph=True)[0]\n",
    "        jacobian.append(jac_i)\n",
    "    return torch.stack(jacobian, dim=1)  # Shape: (batch_size, a_dim, input_dim)\n",
    "\n",
    "\n",
    "def compute_jacobian(network, x):\n",
    "    # Make sure x requires gradients\n",
    "    x = x.clone().detach().requires_grad_(True)\n",
    "    \n",
    "    # Forward pass to get the output a(x)\n",
    "    a = network(x)\n",
    "    \n",
    "    # Initialize an empty list to store the Jacobian\n",
    "    jacobian = []\n",
    "    \n",
    "    # Compute the Jacobian for each output with respect to the input\n",
    "    for i in range(a.shape[1]):\n",
    "        # Create a tensor of the same shape as a, filled with zeros except for the i-th column\n",
    "        grad_output = torch.zeros_like(a)\n",
    "        grad_output[:, i] = 1.0\n",
    "        \n",
    "        # Compute the gradient of the i-th output w.r.t. x\n",
    "        jac_i = torch.autograd.grad(outputs=a, inputs=x,\n",
    "                                    grad_outputs=grad_output, create_graph=True)[0]\n",
    "        \n",
    "        jacobian.append(jac_i)\n",
    "    \n",
    "    # Stack the computed gradients along a new dimension to form the Jacobian\n",
    "    return torch.stack(jacobian, dim=1)  # Shape: (batch_size, a_dim, input_dim)\n",
    "\n",
    "\n",
    "# Initialize dimensions\n",
    "input_dim = 3  # Lorenz system state: (x, y, z)\n",
    "a_dim = 64    # High-dimensional a(x)\n",
    "output_dim = 3 # Output dimension should match the state dimension\n",
    "\n",
    "# Initialize models\n",
    "mapping_net = MappingNet(input_dim, output_dim=a_dim)\n",
    "linear_params = LinearParams(input_dim=a_dim)\n",
    "\n",
    "# Optimizers for both parts\n",
    "optimizer_a = optim.Adam(mapping_net.parameters(), lr=1e-3)\n",
    "optimizer_linear = optim.Adam(linear_params.parameters(), lr=1e-3)\n",
    "\n",
    "# Loss function\n",
    "mse_loss = nn.MSELoss()\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 3000\n",
    "linear_steps = 5  # Number of linear optimizer steps per epoch\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "\n",
    "    # Generate more random input state data\n",
    "    x = torch.randn((256, input_dim), requires_grad=True)  # More states (x, y, z) samples\n",
    "    \n",
    "    for _ in range(linear_steps):\n",
    "        # Step 1: Optimize A and b\n",
    "        optimizer_a.zero_grad()\n",
    "        \n",
    "        # Forward pass through the network\n",
    "        a = mapping_net(x)\n",
    "        \n",
    "        # Recalculate the Jacobian of a(x)\n",
    "        #Ja = compute_jacobian(a, x)\n",
    "        Ja = compute_jacobian(mapping_net, x)\n",
    "        \n",
    "        # Recalculate the left and right sides\n",
    "        left_side = torch.einsum('bij,bj->bi', Ja, F(x))\n",
    "        right_side = linear_params(a)\n",
    "        #left_side = F(x)\n",
    "        #right_side =  torch.einsum('bij,bj->bi', pinv(Ja), linear_params(mapping_net(x)))\n",
    "        \n",
    "        # Compute the loss for a(x)\n",
    "        loss = mse_loss(left_side, right_side)\n",
    "        loss.backward(retain_graph=True)\n",
    "        optimizer_a.step()\n",
    "    \n",
    "    # Step 2: Optimize a(x)\n",
    "    optimizer_linear.zero_grad()\n",
    "    \n",
    "    # Forward pass through the network\n",
    "    a = mapping_net(x)\n",
    "    \n",
    "    # Recalculate the Jacobian of a(x)\n",
    "    #Ja = compute_jacobian(a, x)\n",
    "    Ja = compute_jacobian(mapping_net, x)\n",
    "    \n",
    "    # Recalculate the left and right sides\n",
    "    left_side = torch.einsum('bij,bj->bi', Ja, F(x))\n",
    "    right_side = linear_params(a)\n",
    "    #left_side = F(x)\n",
    "    #right_side =  torch.einsum('bij,bj->bi', pinv(Ja), linear_params(mapping_net(x)))\n",
    "    \n",
    "    # Compute the loss for a(x)\n",
    "    loss_a = mse_loss(left_side, right_side)\n",
    "    loss_a.backward(retain_graph=True)\n",
    "    optimizer_linear.step()\n",
    "\n",
    "    #Javi = pinv(compute_jacobian(mapping_net(xv), xv))\n",
    "    #left_side = torch.einsum('bij,bj->bi', Javi, mapping_net(xv))\n",
    "\n",
    "    #print(left_side, F(xv))\n",
    "    \n",
    "    # Print loss every 100 epochs\n",
    "    if epoch % 100 == 0:\n",
    "        print(f'Epoch {epoch}, Loss (a): {loss_a.item()}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "ea0640c8-9016-4ae2-9e4e-d9ddc3c8d2a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "res =  torch.einsum('bij,bj->bi', (compute_jacobian(mapping_net, x)), F(x)) - linear_params(mapping_net(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "5f2c0975-284a-43b8-90ea-74779d79f886",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 5.5608e-04,  4.8520e-05,  5.4945e-05,  ...,  2.3366e-04,\n",
       "          7.4381e-05,  8.4932e-06],\n",
       "        [ 8.9772e-04, -3.0435e-04, -2.4129e-04,  ..., -7.5167e-04,\n",
       "         -5.2047e-04,  2.0322e-04],\n",
       "        [ 6.4170e-04,  3.9267e-05, -2.0168e-05,  ...,  2.3327e-06,\n",
       "          2.2807e-05, -2.5406e-04],\n",
       "        ...,\n",
       "        [ 6.5218e-04, -1.9912e-05, -3.8731e-05,  ..., -5.1633e-05,\n",
       "         -5.6250e-05, -1.2657e-04],\n",
       "        [ 6.9821e-04,  3.8606e-05, -6.2327e-05,  ..., -8.8867e-05,\n",
       "         -7.5784e-05, -1.5930e-04],\n",
       "        [ 6.5161e-04,  2.3288e-05, -3.0990e-05,  ..., -3.4820e-06,\n",
       "         -4.2515e-05, -1.0845e-04]], grad_fn=<SubBackward0>)"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87ec97ac-6331-46f6-836b-2b7170251991",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
